{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start a simple Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test2\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Walmart Stock CSV File, have Spark infer the data types.\n",
    "df = spark.read.option(\"header\", True).csv(\"walmart_stock-1.csv\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "#What are the column names?\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What does the Schema look like?\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|\n",
      "+----------+------------------+------------------+------------------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004|\n",
      "+----------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print out the first 5 columns\n",
    "fiveColDf = df.select(df.columns[:5])\n",
    "fiveColDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use describe() to learn about the DataFrame.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|      Date|     Open|     High|  Low|    Close| Volume|Adj Close|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "|2015-01-13|90.800003|90.970001|88.93|89.309998|8215400|83.825448|\n",
      "+----------+---------+---------+-----+---------+-------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What day had the Peak High in Price?\n",
    "#df.agg({'High': 'max'}).show()\n",
    "df.orderBy(df.High.desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the mean of the Close column?\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(df.Close)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Volume)|\n",
      "+-----------+\n",
      "|   80898100|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(Volume)|\n",
      "+-----------+\n",
      "|    2094900|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the max and min of the Volume column?\n",
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import min\n",
    "df.select(max(df.Volume)).show()\n",
    "df.select(min(df.Volume)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "#How many days was the Close lower than 60 dollars?\n",
    "lowerThanSixty = df.where(df.Close < 60)\n",
    "row_count = lowerThanSixty.count()\n",
    "print(row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.141494435612083\n"
     ]
    }
   ],
   "source": [
    "#What percentage of the time was the High greater than 80 dollars ? In other words, (Number of Days High>80)/(Total Days in the dataset)\n",
    "higherThanEighty = df.filter(df.High > 80)\n",
    "daysHigh = higherThanEighty.count()\n",
    "totalDays = df.count()\n",
    "print(daysHigh/totalDays*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3384326061737161\n"
     ]
    }
   ],
   "source": [
    "#What is the Pearson correlation between High and Volume?\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "pearsonCorr = df.corr(\"High\", \"Volume\", \"pearson\")\n",
    "print(pearsonCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year(Date)|max(High)|\n",
      "+----------+---------+\n",
      "|      2015|90.970001|\n",
      "|      2013|81.370003|\n",
      "|      2014|88.089996|\n",
      "|      2012|77.599998|\n",
      "|      2016|75.190002|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the max High per year?\n",
    "from pyspark.sql.functions import (dayofmonth, hour,\n",
    "                                   dayofyear, month,\n",
    "                                   year, weekofyear,\n",
    "                                   format_number, date_format)\n",
    "maxHighPerYear = df.groupBy(year(\"Date\")).agg(max(\"High\")).alias(\"year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|            Close|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "#What is the average Close for each Calendar Month?\n",
    "#In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n",
    "avgClosePerMonth = df.groupBy(month(\"Date\").alias(\"Month\")).agg(avg(\"Close\").alias(\"Close\")).orderBy(\"Month\")\n",
    "#Create a new column Month from existing Date column\n",
    "df = df.withColumn(\"Month\", month(\"Date\"))\n",
    "\n",
    "\n",
    "#Group by month and take average of all other columns\n",
    "avgAllPerMonth = df.groupBy(\"Month\").agg(avg(\"High\").alias(\"High\"), avg(\"Low\").alias(\"Low\"), avg(\"Close\").alias(\"Close\"), avg(\"Volume\").alias(\"Volume\"), avg(\"Adj Close\").alias(\"Adj Close\"))\n",
    "\n",
    "#Sort by month\n",
    "avgAllPerMonth = avgAllPerMonth.orderBy(\"Month\")\n",
    "\n",
    "#Display only month and avg(Close), the desired columns\n",
    "avgAllPerMonth.select(\"Month\", \"Close\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
